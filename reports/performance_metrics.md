# Performance Metrics Report  
Spark Structured Streaming → PostgreSQL

These are the observed performance of the real-time data ingestion
pipeline.

---

## Test Setup
- Source: CSV files generated by a Python event generator
- Ingestion: Spark Structured Streaming (file-based)
- Trigger interval: 10 seconds
- Sink: PostgreSQL via JDBC (`foreachBatch`)
- Checkpointing: Enabled
- Execution environment: Local machine (WSL2)
- Measurement window: ~60 seconds steady-state run

---

## Performance Metrics Summary

| Metric | Description | Observed Value | Measurement Method |
|------|------------|---------------|-------------------|
| **Configured Input Rate** | Events produced by generator | ~20 events/sec | `(files_per_min × rows_per_file) / 60` |
| **Observed Throughput** | Records written to PostgreSQL | ~23 rows/sec | `COUNT(*) / 60s` from Postgres |
| **Batch Size** | Records per Spark micro-batch | 200–600 rows ( Varies per batch) | Spark console logs |
| **Batch Processing Time** | Time to process one batch | ~0.3–3.7 seconds | Spark console logs |
| **Average Latency** | Event creation → DB insertion | ~121 seconds | `AVG(ingest_time - event_time)` |
| **Maximum Latency** | Worst-case end-to-end delay | ~209 seconds | `MAX(ingest_time - event_time)` |
| **Error Rate** | Failed batches or records | 0% observed | Spark logs and absence of write exceptions |

---

## Latency Interpretation and Measurement
Latency is defined as the time difference between:
- `event_time`: timestamp assigned at event generation
- `ingest_time`: timestamp assigned when the record is written to PostgreSQL

It is measured in seconds using the following SQL expression:
```sql
EXTRACT(EPOCH FROM (ingest_time - event_time))
```
